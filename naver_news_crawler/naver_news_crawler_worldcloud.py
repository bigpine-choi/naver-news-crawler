import requests
from bs4 import BeautifulSoup
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from konlpy.tag import Okt
from collections import Counter
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import logging
import time

# âœ… ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# âœ… ê¸°ë³¸ ì„¤ì •ê°’
BASE_URL = "https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=101&date={date}&page={page}"

# âœ… ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
DEFAULT_STOPWORDS = {
    "ê¸°ì", "ì§€ë‚œí•´", "ì§€ì›", "ê¸°ì—…", "ìµœëŒ€", "ì—°íœ´", "ì—­ëŒ€", "ì‚¬ì—…", "ëŒ€í•œ", "ì´ë²ˆ", "ê´€ë ¨", "ëŒ€í•´", "ë“±ì˜", "ì§€ë‚œ", "ì˜¤ëŠ˜",
    "ë‚´ì¼", "ì˜¬í•´", "ê²½ìš°", "ìƒˆë¡œìš´", "ë‰´ìŠ¤", "ê²½ì œ", "ë³´ë„", "ëŒ€í•œë¯¼êµ­", "ì •ë¶€", "ê¸ˆìœµ", "ì‹œì¥", "í¬í† ", "í•œêµ­", "ì†ë³´", "ìœ„í•´",
    "ì‘ë…„", "íˆ¬ì", "ê°œì›”", "ë¸Œëœë“œ"
}

# âœ… ìë™ ì œê±°ì—ì„œ ë³µì›í•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
EXCLUDED_KEYWORDS = {"íŠ¸ëŸ¼í”„", "ì‚¼ì„±", "ì „ìŸ", "ì‹œí¬", "ê´€ì„¸", "í•˜ì´ë‹‰ìŠ¤", "ì•„íŒŒíŠ¸", "ì„¸ì¢…", "ëŒ€ì™•ê³ ë˜", "ì¸í•˜", "ì ‘ì†", "ì°¨ë‹¨", "ì™¸êµ", "ì‚°ì—…ë¶€", "ëŒ€ì¶œ", "ì˜¬íŠ¸ë¨¼", "ì œì£¼í•­ê³µ", "ê³ ë ¤ì•„ì—°"}

# âœ… íŠ¹ì • ë‚ ì§œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_last_page(session, date):
    """ íŠ¹ì • ë‚ ì§œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
    url = BASE_URL.format(date=date, page=1)
    try:
        response = session.get(url, timeout=5)  # âœ… 5ì´ˆ ì œí•œ
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "lxml")

        # âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
        page_numbers = [
            int(a.get_text(strip=True)) for a in soup.select(".paging a")
            if a.get_text(strip=True).isdigit()
        ]
        return max(page_numbers) if page_numbers else 5  # âœ… ê¸°ë³¸ê°’ 5í˜ì´ì§€

    except requests.RequestException as e:
        logging.error(f"âŒ í˜ì´ì§€ ìˆ˜ í™•ì¸ ì‹¤íŒ¨ ({date}): {e}")
        return 5  # âœ… ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ 5í˜ì´ì§€

# âœ… ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜ (ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ì‹œë„ ê¸°ëŠ¥ ì¶”ê°€)
def fetch_news(session, date, page, retries=3):
    """ íŠ¹ì • ë‚ ì§œì™€ í˜ì´ì§€ì˜ ë‰´ìŠ¤ ì œëª©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ìµœëŒ€ 3ë²ˆ ì¬ì‹œë„) """
    url = BASE_URL.format(date=date, page=page)
    for attempt in range(retries):
        try:
            response = session.get(url, timeout=5)  # âœ… 5ì´ˆ íƒ€ì„ì•„ì›ƒ
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")

            headlines = [
                headline.get_text(strip=True)
                for headline in soup.select(".list_body.newsflash_body li dt:not(.photo) a")
            ]

            return headlines if headlines else None

        except requests.Timeout:
            logging.warning(f"â³ íƒ€ì„ì•„ì›ƒ ë°œìƒ ({date}, í˜ì´ì§€ {page}) - ì¬ì‹œë„ {attempt + 1}/{retries}")
        except requests.RequestException as e:
            logging.error(f"âŒ ìš”ì²­ ì‹¤íŒ¨ ({date}, í˜ì´ì§€ {page}) - ì¬ì‹œë„ {attempt + 1}/{retries}: {e}")

        time.sleep(1)  # âœ… ì¬ì‹œë„ ì „ 1ì´ˆ ëŒ€ê¸°

    logging.error(f"ğŸš¨ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {date}, í˜ì´ì§€ {page}")
    return None

# âœ… ê¸°ê°„ë³„ í¬ë¡¤ë§ (ë™ì  í˜ì´ì§€ í¬ë¡¤ë§)
def get_news_titles_by_date(start_date, end_date):
    """ íŠ¹ì • ê¸°ê°„ ë™ì•ˆì˜ ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ì„ í¬ë¡¤ë§ (ë™ì  í˜ì´ì§€ ì²˜ë¦¬) """
    news_titles = set()  # âœ… ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set ì‚¬ìš©
    max_workers = min(10, os.cpu_count() or 4)

    with requests.Session() as session:
        session.headers.update({"User-Agent": "Mozilla/5.0"})

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            tasks = []
            date_cursor = start_date

            while date_cursor <= end_date:
                formatted_date = date_cursor.strftime("%Y%m%d")
                logging.info(f"ğŸ“… {formatted_date} ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘...")

                # âœ… ì²« í˜ì´ì§€ ìš”ì²­ ë° ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸
                first_page_titles = fetch_news(session, formatted_date, 1)
                if first_page_titles:
                    news_titles.update(first_page_titles)

                    # âœ… í•´ë‹¹ ë‚ ì§œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°
                    last_page = get_last_page(session, formatted_date)

                    # âœ… 2í˜ì´ì§€ë¶€í„° ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬)
                    tasks += [executor.submit(fetch_news, session, formatted_date, page) for page in range(2, last_page + 1)]

                date_cursor += timedelta(days=1)

            # âœ… ë³‘ë ¬ ìš”ì²­ ê²°ê³¼ ì²˜ë¦¬
            for future in as_completed(tasks):
                result = future.result()
                if result:
                    news_titles.update(result)

    return list(news_titles)  # âœ… ìµœì¢… ë¦¬ìŠ¤íŠ¸ ë³€í™˜

# âœ… ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜ (ë¹ˆë„ìˆ˜ ë†’ì€ ë‹¨ì–´ ìë™ ì œê±° ì¶”ê°€, ì˜ˆì™¸ ì²˜ë¦¬)
def create_wordcloud(news_titles, top_n_stopwords=5):
    """ í¬ë¡¤ë§í•œ ë‰´ìŠ¤ ì œëª©ì„ í™œìš©í•˜ì—¬ ê°€ë…ì„± ë†’ì€ ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ """
    okt = Okt()

    # âœ… ëª…ì‚¬ ì¶”ì¶œ + ë¶ˆìš©ì–´ ì œê±° + í•œ ê¸€ì ë‹¨ì–´ ì œì™¸
    words = [noun for title in news_titles for noun in okt.nouns(title)
             if noun not in DEFAULT_STOPWORDS and len(noun) > 1]

    word_freq = Counter(words)

    # âœ… ìƒìœ„ Nê°œ ë‹¨ì–´ ìë™ ë¶ˆìš©ì–´ ì¶”ê°€ (ë‹¨, EXCLUDED_KEYWORDSëŠ” ì œì™¸)
    common_words = {word for word, _ in word_freq.most_common(top_n_stopwords) if word not in EXCLUDED_KEYWORDS}
    filtered_word_freq = {word: freq for word, freq in word_freq.items() if word not in common_words}

    logging.info(f"ğŸ›‘ ìë™ìœ¼ë¡œ ì œì™¸ëœ ìƒìœ„ {top_n_stopwords}ê°œ ë‹¨ì–´ (ë‹¨, {EXCLUDED_KEYWORDS} ì œì™¸): {common_words}")

    # âœ… ê°œì„ ëœ ì›Œë“œ í´ë¼ìš°ë“œ ì„¤ì •
    wordcloud = WordCloud(
        font_path="C:/Windows/Fonts/malgun.ttf",
        background_color="white",
        width=1000,
        height=600,
        max_words=150,
        colormap="Dark2",
        prefer_horizontal=1.0,
        relative_scaling=0.5
    ).generate_from_frequencies(filtered_word_freq)

    # âœ… ì›Œë“œ í´ë¼ìš°ë“œ ì¶œë ¥
    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# âœ… ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    start_date = datetime(2025, 2, 1)
    end_date = datetime(2025, 2, 8)

    news_titles = get_news_titles_by_date(start_date, end_date)

    logging.info(f"\nğŸ“¢ ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ({start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}):")
    print("\n".join(f"{i + 1}. {title}" for i, title in enumerate(news_titles[:10])))

    create_wordcloud(news_titles)