import requests
from bs4 import BeautifulSoup
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from konlpy.tag import Okt
from collections import Counter
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import logging

# âœ… ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# âœ… ê¸°ë³¸ ì„¤ì •ê°’
BASE_URL = "https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=101&date={date}&page={page}"

# âœ… ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
DEFAULT_STOPWORDS = {
    "ê¸°ì", "ì´", "ê·¸", "ê²ƒ", "ì €", "ë“±", "ë°", "ì¤‘", "ëŒ€í•œ", "ì´ë²ˆ", "ê´€ë ¨",
    "ìˆ˜", "ë”", "ë¡œ", "ìœ„", "ëŒ€í•´", "ë“±ì˜", "ì§€ë‚œ", "ì˜¤ëŠ˜", "ë‚´ì¼", "ì˜¬í•´",
    "ê²½ìš°", "ìƒˆë¡œìš´", "ë‰´ìŠ¤", "ê²½ì œ", "ë³´ë„", "ëŒ€í•œë¯¼êµ­", "ì •ë¶€", "ê¸ˆìœµ", "ì‹œì¥",
    "í¬í† ", "í•œêµ­", "ì†ë³´", "ìœ„í•´", "ì¤‘ì•™", "ì„œìš¸", "ëŒ€í†µë ¹", "ì „êµ­", "ì‚¬ëŒ", "ì˜ì›"
}

# âœ… ìë™ ì œê±°ì—ì„œ ë³µì›í•  ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
EXCLUDED_KEYWORDS = {"íŠ¸ëŸ¼í”„"}

# âœ… íŠ¹ì • ë‚ ì§œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_last_page(session, date):
    """ íŠ¹ì • ë‚ ì§œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
    url = BASE_URL.format(date=date, page=1)
    try:
        response = session.get(url, timeout=5)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "lxml")

        # âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
        page_numbers = [
            int(a.get_text(strip=True)) for a in soup.select(".paging a")
            if a.get_text(strip=True).isdigit()
        ]
        return max(page_numbers) if page_numbers else 5  # âœ… ê¸°ë³¸ê°’ 5í˜ì´ì§€

    except requests.RequestException as e:
        logging.error(f"âŒ í˜ì´ì§€ ìˆ˜ í™•ì¸ ì‹¤íŒ¨ ({date}): {e}")
        return 5  # âœ… ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ 5í˜ì´ì§€

# âœ… ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬)
def fetch_news(session, date, page):
    """ íŠ¹ì • ë‚ ì§œì™€ í˜ì´ì§€ì˜ ë‰´ìŠ¤ ì œëª©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ """
    url = BASE_URL.format(date=date, page=page)
    try:
        response = session.get(url, timeout=5)  # âœ… 5ì´ˆ íƒ€ì„ì•„ì›ƒ ì¶”ê°€
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "lxml")

        headlines = [
            headline.get_text(strip=True)
            for headline in soup.select(".list_body.newsflash_body li dt:not(.photo) a")
        ]

        return headlines if headlines else None

    except requests.Timeout:
        logging.error(f"â³ íƒ€ì„ì•„ì›ƒ ë°œìƒ ({date}, í˜ì´ì§€ {page})")
        return None
    except requests.RequestException as e:
        logging.error(f"âŒ ìš”ì²­ ì‹¤íŒ¨ ({date}, í˜ì´ì§€ {page}): {e}")
        return None

# âœ… ê¸°ê°„ë³„ í¬ë¡¤ë§ (ë™ì  í˜ì´ì§€ í¬ë¡¤ë§)
def get_news_titles_by_date(start_date, end_date):
    """ íŠ¹ì • ê¸°ê°„ ë™ì•ˆì˜ ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©ì„ í¬ë¡¤ë§ (ë™ì  í˜ì´ì§€ ì²˜ë¦¬) """
    news_titles = []
    max_workers = min(10, os.cpu_count() or 4)

    with requests.Session() as session:
        session.headers.update({"User-Agent": "Mozilla/5.0"})

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            tasks = []
            date_cursor = start_date

            while date_cursor <= end_date:
                formatted_date = date_cursor.strftime("%Y%m%d")
                logging.info(f"ğŸ“… {formatted_date} ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘...")

                # âœ… ì²« í˜ì´ì§€ ìš”ì²­ ë° ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸
                first_page_titles = fetch_news(session, formatted_date, 1)
                if first_page_titles:
                    news_titles.extend(first_page_titles)

                    # âœ… í•´ë‹¹ ë‚ ì§œì˜ ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°
                    last_page = get_last_page(session, formatted_date)

                    # âœ… 2í˜ì´ì§€ë¶€í„° ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬)
                    tasks += [executor.submit(fetch_news, session, formatted_date, page) for page in range(2, last_page + 1)]

                date_cursor += timedelta(days=1)

            # âœ… ë³‘ë ¬ ìš”ì²­ ê²°ê³¼ ì²˜ë¦¬
            for future in as_completed(tasks):
                result = future.result()
                if result:
                    news_titles.extend(result)

    return list(set(news_titles))  # âœ… ì¤‘ë³µ ì œê±°

# âœ… ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜ (ë¹ˆë„ìˆ˜ ë†’ì€ ë‹¨ì–´ ìë™ ì œê±° ì¶”ê°€, ì˜ˆì™¸ ì²˜ë¦¬)
def create_wordcloud(news_titles, top_n_stopwords=5):
    """ í¬ë¡¤ë§í•œ ë‰´ìŠ¤ ì œëª©ì„ í™œìš©í•˜ì—¬ ê°€ë…ì„± ë†’ì€ ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ """
    okt = Okt()

    # âœ… ëª…ì‚¬ ì¶”ì¶œ + ë¶ˆìš©ì–´ ì œê±° + í•œ ê¸€ì ë‹¨ì–´ ì œì™¸
    words = [noun for title in news_titles for noun in okt.nouns(title)
             if noun not in DEFAULT_STOPWORDS and len(noun) > 1]

    word_freq = Counter(words)

    # âœ… ìƒìœ„ Nê°œ ë‹¨ì–´ ìë™ ë¶ˆìš©ì–´ ì¶”ê°€ (ë‹¨, EXCLUDED_KEYWORDSëŠ” ì œì™¸)
    common_words = {word for word, _ in word_freq.most_common(top_n_stopwords) if word not in EXCLUDED_KEYWORDS}
    filtered_word_freq = {word: freq for word, freq in word_freq.items() if word not in common_words}

    logging.info(f"ğŸ›‘ ìë™ìœ¼ë¡œ ì œì™¸ëœ ìƒìœ„ {top_n_stopwords}ê°œ ë‹¨ì–´ (ë‹¨, {EXCLUDED_KEYWORDS} ì œì™¸): {common_words}")

    # âœ… ê°œì„ ëœ ì›Œë“œ í´ë¼ìš°ë“œ ì„¤ì •
    wordcloud = WordCloud(
        font_path="C:/Windows/Fonts/malgun.ttf",
        background_color="white",
        width=1000,
        height=600,
        max_words=150,
        colormap="Dark2",
        prefer_horizontal=1.0,
        relative_scaling=0.5
    ).generate_from_frequencies(filtered_word_freq)

    # âœ… ì›Œë“œ í´ë¼ìš°ë“œ ì¶œë ¥
    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis("off")
    plt.show()

# âœ… ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    start_date = datetime(2025, 1, 1)
    end_date = datetime(2025, 2, 7)

    news_titles = get_news_titles_by_date(start_date, end_date)

    logging.info(f"\nğŸ“¢ ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ({start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}):")
    print("\n".join(f"{i + 1}. {title}" for i, title in enumerate(news_titles[:20])))

    create_wordcloud(news_titles)